# -*- coding: utf-8 -*-
"""Positive_EV

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jKzxjNoiTYIMZcE--GhhPFXCVgmhB5nA

#Run
"""

import requests
import csv
import pandas as pd
import numpy as np
from datetime import datetime
import re
from io import StringIO

# --- Helper Functions ---
def fetch_json(url):
    """Fetch JSON data from a URL with headers."""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36",
        "Accept": "application/json, text/javascript, */*; q=0.01",
        "Referer": "https://www.actionnetwork.com",
        "Origin": "https://www.actionnetwork.com",
    }
    response = requests.get(url, headers=headers)
    response.raise_for_status()
    return response.json()

def load_book_mapping(csv_path="/content/drive/MyDrive/Positive_ev/book_weights.csv"):
    """Load book ID to name and weight mapping from CSV."""
    try:
        df = pd.read_csv(csv_path)
        if all(col in df.columns for col in ['Book ID', 'Name', 'Weight']):
            book_id_mapping = dict(zip(df['Book ID'].astype(str), df['Name']))
            book_weight_mapping = dict(zip(df['Book ID'].astype(str), df['Weight'].astype(str)))
            print(f"Loaded {len(book_id_mapping)} book mappings from {csv_path}")
            return book_id_mapping, book_weight_mapping
        print(f"Warning: Expected columns missing in {csv_path}. Available: {list(df.columns)}")
        return {}, {}
    except FileNotFoundError:
        print(f"Warning: {csv_path} not found.")
        return {}, {}
    except Exception as e:
        print(f"Error loading book mappings: {e}")
        return {}, {}

def extract_outcomes(data):
    """Extract betting outcomes recursively from data structure."""
    outcomes = []
    def recursive_extract(obj, parent_keys=None):
        parent_keys = parent_keys or []
        if isinstance(obj, dict):
            if "outcome_id" in obj or ("odds" in obj and "book_id" in obj):
                outcomes.append(obj)
            for key, value in obj.items():
                if key in ["odds", "lines", "markets", "betting", "books", "sportsbooks", "outcomes"]:
                    recursive_extract(value, parent_keys + [key])
                elif isinstance(value, (dict, list)):
                    recursive_extract(value, parent_keys + [key])
        elif isinstance(obj, list):
            for item in obj:
                recursive_extract(item, parent_keys)
    recursive_extract(data)
    return outcomes

def build_event_lookup(data):
    """Build event ID to team names and logos lookup."""
    event_lookup = {}
    def process_games(obj):
        if isinstance(obj, dict):
            if all(k in obj for k in ["id", "home_team_id", "away_team_id"]) and "teams" in obj:
                home = next((t for t in obj["teams"] if t["id"] == obj["home_team_id"]), None)
                away = next((t for t in obj["teams"] if t["id"] == obj["away_team_id"]), None)
                if home and away:
                    event_lookup[obj["id"]] = {
                        "home_team": home.get("full_name", home.get("name", "")),
                        "away_team": away.get("full_name", away.get("name", "")),
                        "home_logo": home.get("logo", ""),
                        "away_logo": away.get("logo", "")
                    }
            for value in obj.values():
                if isinstance(value, (dict, list)):
                    process_games(value)
        elif isinstance(obj, list):
            for item in obj:
                process_games(item)
    process_games(data)
    return event_lookup

def save_to_csv(outcomes, event_lookup, book_id_mapping, book_weight_mapping, filename):
    """Save outcomes to CSV with team data, book names, and weights, filtering by market_type."""
    fields = [
        "event_id", "home_team", "away_team", "home_logo", "away_logo", "book_id", "book_name",
        "market_type", "outcome_id", "event_type", "type", "side", "period", "team_id", "odds",
        "value", "is_live", "line_status", "tickets_value", "tickets_percent", "money_value",
        "money_percent", "odds_coefficient_score", "weight"
    ]
    with open(filename, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fields)
        writer.writeheader()
        for outcome in outcomes:
            market_type = outcome.get("type", outcome.get("market_type", "")).lower()
            if market_type not in ["moneyline", "total", "spread"]:
                continue # Skip outcomes that are not moneyline, total, or spread

            event_id = outcome.get("event_id", "")
            team_data = event_lookup.get(event_id, {"home_team": "", "away_team": "", "home_logo": "", "away_logo": ""})
            bet_info = outcome.get("bet_info", {})
            tickets = bet_info.get("tickets", {}) if isinstance(bet_info, dict) else outcome.get("tickets", {})
            money = bet_info.get("money", {}) if isinstance(bet_info, dict) else outcome.get("money", {})
            book_id = str(outcome.get("book_id", ""))
            row = {
                "event_id": event_id,
                "home_team": team_data["home_team"],
                "away_team": team_data["away_team"],
                "home_logo": team_data["home_logo"],
                "away_logo": team_data["away_logo"],
                "book_id": book_id,
                "book_name": book_id_mapping.get(book_id, f"Unknown_{book_id}"),
                "market_type": outcome.get("type", outcome.get("market_type", "")),
                "outcome_id": outcome.get("outcome_id", ""),
                "event_type": outcome.get("event_type", ""),
                "type": outcome.get("type", ""),
                "side": outcome.get("side", ""),
                "period": outcome.get("period", ""),
                "team_id": outcome.get("team_id", ""),
                "odds": outcome.get("odds", ""),
                "value": outcome.get("value", ""),
                "is_live": outcome.get("is_live", ""),
                "line_status": outcome.get("line_status", ""),
                "tickets_value": tickets.get("value", 0) if isinstance(tickets, dict) else 0,
                "tickets_percent": tickets.get("percent", 0) if isinstance(tickets, dict) else 0,
                "money_value": money.get("value", 0) if isinstance(money, dict) else 0,
                "money_percent": money.get("percent", 0) if isinstance(money, dict) else 0,
                "odds_coefficient_score": outcome.get("odds_coefficient_score", ""),
                "weight": book_weight_mapping.get(book_id, "N/A")
            }
            writer.writerow(row)

def format_date(date_input=None):
    """Convert date to YYYYMMDD format."""
    if date_input is None:
        return datetime.now().strftime("%Y%m%d")
    if isinstance(date_input, datetime):
        return date_input.strftime("%Y%m%d")
    try:
        for fmt in ["%Y-%m-%d", "%m/%d/%Y", "%Y%m%d"]:
            try:
                return datetime.strptime(date_input, fmt).strftime("%Y%m%d")
            except ValueError:
                continue
        raise ValueError(f"Invalid date format: {date_input}")
    except ValueError:
        raise ValueError(f"Date must be string or datetime, got {type(date_input)}")

# --- Data Fetching ---
def fetch_mlb_data(target_date=None, books_csv_path="/content/drive/MyDrive/Positive_ev/book_weights.csv"):
    """Fetch MLB betting data and save to CSV."""
    formatted_date = format_date(target_date)
    print(f"Processing data for date: {formatted_date}")

    # Load book mappings from CSV
    book_id_mapping, book_weight_mapping = load_book_mapping(books_csv_path)

    # Generate book_ids string from the loaded mappings
    if book_id_mapping:
        book_ids = ",".join(book_id_mapping.keys())
    else:
        print("Warning: No book mappings loaded, using empty book_ids")
        book_ids = ""
    print(f"Fetching data for book IDs: {book_ids}")

    url = f"https://api.actionnetwork.com/web/v2/scoreboard/publicbetting/mlb?bookIds={book_ids}&date={formatted_date}&periods=event"

    print("Fetching MLB data...")
    data = fetch_json(url)

    outcomes = extract_outcomes(data)
    event_lookup = build_event_lookup(data)
    print(f"Found {len(outcomes)} raw outcomes, {len(event_lookup)} events")

    filename = f"mlb_betting_data_weighted_{formatted_date}.csv"
    save_to_csv(outcomes, event_lookup, book_id_mapping, book_weight_mapping, filename)
    print(f"Data saved to {filename}")
    return filename

# --- Odds Processing ---
def american_to_decimal(american_odds):
    """Convert American odds to decimal odds."""
    if american_odds > 0:
        return (american_odds / 100) + 1
    elif american_odds < 0:
        return (100 / abs(american_odds)) + 1
    else:
        return 1.0

def decimal_to_american(decimal_odds):
    """Convert decimal odds to American odds."""
    if decimal_odds >= 2.0:
        american = (decimal_odds - 1) * 100
        return f"+{american:.0f}"
    elif decimal_odds > 1.0:
        american = -100 / (decimal_odds - 1)
        return f"{american:.0f}"
    else:
        return "+100"  # Fallback for edge cases

def parse_single_record(record):
    """Parse a single concatenated record."""
    event_id_match = re.match(r'^(\d{6})', record)
    if not event_id_match:
        return None
    event_id = event_id_match.group(1)

    odds_match = re.search(r'[+-]\d{3,}', record)
    odds = int(odds_match.group(0)) if odds_match else 0

    single_digits = [int(n) for n in re.findall(r'(?<!\d)\d(?!\d)', record) if 0 <= int(n) <= 10]
    weight = single_digits[-1] if single_digits else 0

    teams = re.findall(r'[A-Z][a-z]+ [A-Z][a-z]+|away_team|home_team', record)
    home_team = 'unknown'
    away_team = 'unknown'
    for team in teams:
        if team not in ['away_team', 'home_team']:
            if home_team == 'unknown':
                home_team = team
            elif away_team == 'unknown':
                away_team = team
            break

    bet_type = re.search(r'(moneyline|total|spread)', record, re.IGNORECASE)
    bet_type = bet_type.group(1).lower() if bet_type else 'unknown'

    side_match = re.search(r'(over|under)', record, re.IGNORECASE)
    side = side_match.group(1).lower() if side_match else 'unknown'

    # Extract 'value' from the record
    value_match = re.search(r'(?:value|val)[:_=\s]*([-+]?\d*\.?\d+)', record, re.IGNORECASE)
    value = float(value_match.group(1)) if value_match else None

    return {
        'event_id': event_id,
        'home_team': home_team,
        'away_team': away_team,
        'type': bet_type,
        'side': side,
        'odds': odds,
        'weight': min(max(weight, 0), 10),
        'value': value
    }

def parse_file(file_path):
    """Parse file as CSV or custom format."""
    try:
        # Attempt to read as a standard CSV
        df = pd.read_csv(file_path)
        required_cols = ['event_id', 'home_team', 'away_team', 'type', 'side', 'odds', 'weight', 'value']
        if all(col in df.columns for col in required_cols):
            print(f"Successfully loaded CSV from {file_path}")
            return df
        else:
            print(f"CSV missing required columns. Trying alternative parsing. Missing: {[col for col in required_cols if col not in df.columns]}")
            raise ValueError("Missing columns")

    except (pd.errors.EmptyDataError, FileNotFoundError, ValueError) as e:
        print(f"Failed to load as standard CSV ({e}). Attempting custom parsing.")
        try:
            # Try parsing as tab-separated
            df = pd.read_csv(file_path, sep='\t')
            required_cols = ['event_id', 'home_team', 'away_team', 'type', 'side', 'odds', 'weight', 'value']
            if all(col in df.columns for col in required_cols):
                print(f"Successfully loaded tab-separated CSV from {file_path}")
                return df
            else:
                print(f"Tab-separated CSV missing required columns. Trying custom format parsing. Missing: {[col for col in required_cols if col not in df.columns]}")
                raise ValueError("Missing columns in tab-separated")

        except (pd.errors.EmptyDataError, FileNotFoundError, ValueError) as e:
            print(f"Failed to load as tab-separated CSV ({e}). Attempting custom concatenated record parsing.")
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            records = [r.strip() for r in re.split(r'(?=\d{6})', content) if r.strip()]
            parsed = [parse_single_record(r) for r in records if r]
            df = pd.DataFrame([p for p in parsed if p])
            print(f"Successfully parsed {len(df)} records using custom format.")
            return df
    except Exception as e:
        print(f"An unexpected error occurred during file parsing: {e}")
        return pd.DataFrame()

def calculate_weighted_averages(df):
    """Calculate weighted averages for odds and include the most common 'value'."""
    print("Starting weighted average calculation...")

    # Clean and validate data
    df['odds'] = pd.to_numeric(df['odds'], errors='coerce').fillna(0)
    df['weight'] = pd.to_numeric(df['weight'], errors='coerce').clip(0, 10).fillna(0)

    # Handle 'value' column
    if 'value' in df.columns:
        if pd.api.types.is_numeric_dtype(df['value']):
            df['value'] = pd.to_numeric(df['value'], errors='coerce').fillna(0)
        else:
            df['value'] = df['value'].fillna('')
    else:
        df['value'] = ''

    # Filter out invalid odds and weights
    df = df[(df['odds'] != 0) & (df['weight'] >= 0) & (df['weight'] <= 10)]
    print(f"After filtering: {len(df)} valid records")

    grouped = df.groupby(['event_id', 'home_team', 'away_team', 'type', 'side'])
    results = []

    for name, group in grouped:
        event_id, home_team, away_team, bet_type, side = name

        print(f"Processing group: {event_id} | {home_team} vs {away_team} | {bet_type} | {side}")
        print(f"  Original odds: {group['odds'].tolist()}")
        print(f"  Weights: {group['weight'].tolist()}")

        # Convert American odds to decimal odds
        decimal_odds = []
        for _, row in group.iterrows():
            decimal_odd = american_to_decimal(row['odds'])
            decimal_odds.append(decimal_odd)

        print(f"  Decimal odds: {decimal_odds}")

        weights = group['weight'].tolist()

        if sum(weights) > 0:
            # Calculate weighted average of decimal odds
            weighted_avg_decimal = sum(d * w for d, w in zip(decimal_odds, weights)) / sum(weights)
            print(f"  Weighted avg decimal: {weighted_avg_decimal}")

            # Convert back to American odds
            weighted_avg_american = decimal_to_american(weighted_avg_decimal)
            print(f"  Weighted avg American: {weighted_avg_american}")
        else:
            weighted_avg_american = "+100"  # Default fallback
            print(f"  No weights, using default: {weighted_avg_american}")

        # Get most common value
        most_common_value = None
        if 'value' in group.columns and not group['value'].empty:
            mode_values = group['value'].mode()
            if len(mode_values) > 0:
                most_common_value = mode_values.iloc[0]

        results.append({
            'event_id': event_id,
            'home_team': home_team,
            'away_team': away_team,
            'type': bet_type,
            'side': side,
            'weighted_average_odds': weighted_avg_american,
            'most_common_value': most_common_value,
            'total_records': len(group),
            'total_weight': sum(weights),
            'individual_odds': group['odds'].tolist(),
            'individual_weights': group['weight'].tolist()
        })

    return pd.DataFrame(results)

def process_odds_file(input_path, output_path):
    """Process odds file and save weighted averages."""
    print(f"Processing {input_path}")
    df = parse_file(input_path)

    if df.empty:
        print("Error: No data parsed or loaded.")
        return

    print(f"Parsed {len(df)} records")
    print("Sample of parsed data:")
    print(df.head().to_string())

    results_df = calculate_weighted_averages(df)
    if results_df.empty:
        print("No valid groups found for weighted average calculation.")
        return

    print(f"Generated {len(results_df)} weighted average groups.")
    print("Sample results of weighted averages:")
    print(results_df[['event_id', 'home_team', 'away_team', 'type', 'side', 'weighted_average_odds', 'most_common_value', 'total_records']].head().to_string())

    for i, row in results_df.head(3).iterrows():
        print(f"\nGroup {i+1}: {row['event_id']} | {row['home_team']} vs {row['away_team']} | {row['type']} | {row['side']}")
        print(f"  Odds: {row['individual_odds']}, Weights: {row['individual_weights']}, Avg: {row['weighted_average_odds']}, Value: {row['most_common_value']}")

    output_df = results_df[['event_id', 'home_team', 'away_team', 'type', 'side', 'weighted_average_odds', 'most_common_value', 'total_records', 'total_weight']]
    output_df.to_csv(output_path, index=False)
    print(f"Results saved to {output_path}")

    positive_odds = sum(1 for x in results_df['weighted_average_odds'] if isinstance(x, str) and x.startswith('+'))
    print(f"\nSummary: {len(results_df)} groups, {positive_odds} positive odds, {len(results_df) - positive_odds} negative odds")
    print(f"Avg records/group: {results_df['total_records'].mean():.2f}, Avg weight/group: {results_df['total_weight'].mean():.2f}")

def process_from_string(data_string, output_path):
    """Process odds data from string."""
    records = [r.strip() for r in re.split(r'(?=\d{6})', data_string) if r.strip()]
    parsed = [parse_single_record(r) for r in records if r]
    df = pd.DataFrame([p for p in parsed if p])

    if df.empty:
        print("No data parsed from string.")
        return pd.DataFrame()

    results_df = calculate_weighted_averages(df)
    results_df.to_csv(output_path, index=False)
    print(f"Results saved to {output_path}")
    return results_df

from datetime import datetime
from zoneinfo import ZoneInfo  # For Python 3.9+

# --- Main Execution ---
if __name__ == "__main__":
    try:
        # Get current date in Eastern Time
        eastern_now = datetime.now(ZoneInfo("America/New_York"))
        date_str = eastern_now.strftime("%Y-%m-%d")

        # Fetch data for today's date (in EST/EDT)
        input_file = fetch_mlb_data(date_str, "/content/drive/MyDrive/Positive_ev/book_weights.csv")

        # Process odds from the fetched CSV file
        process_odds_file(input_file, "weighted_averages_results.csv")
    except FileNotFoundError:
        print(f"Error: Required file not found. Please check paths for book_weights.csv or the generated input_file.")
    except Exception as e:
        print(f"An unexpected error occurred during execution: {e}")

import requests
import csv
import pandas as pd
import numpy as np
from datetime import datetime

# --- Helper Functions ---
def fetch_json(url):
    """Fetch JSON data from a URL with appropriate headers."""
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/114.0.0.0 Safari/537.36"
        ),
        "Accept": "application/json, text/javascript, */*; q=0.01",
        "Referer": "https://www.actionnetwork.com",
        "Origin": "https://www.actionnetwork.com",
    }
    response = requests.get(url, headers=headers)
    response.raise_for_status()
    return response.json()

def load_book_mapping(csv_path="/content/drive/MyDrive/Positive_ev/book_weights.csv"):
    """Load book ID to name mapping from CSV file."""
    book_id_mapping = {}
    try:
        df = pd.read_csv(csv_path)
        if 'book_id' in df.columns and 'book_name' in df.columns:
            book_id_mapping = dict(zip(df['book_id'].astype(str), df['book_name']))
        elif 'id' in df.columns and 'name' in df.columns:
            book_id_mapping = dict(zip(df['id'].astype(str), df['name']))
        elif 'id' in df.columns and 'display_name' in df.columns:
            book_id_mapping = dict(zip(df['id'].astype(str), df['display_name']))
        else:
            print("Warning: Could not find expected columns in book_weights.csv")
            print(f"Available columns: {list(df.columns)}")
            if len(df.columns) >= 2:
                book_id_mapping = dict(zip(df.iloc[:, 0].astype(str), df.iloc[:, 1]))
                print(f"Using first two columns: {df.columns[0]} -> {df.columns[1]}")
            else:
                print("Error: book_weights.csv must have at least 2 columns")
                return {}
        print(f"Loaded {len(book_id_mapping)} book mappings from {csv_path}")
    except FileNotFoundError:
        print(f"Error: {csv_path} not found. This file is required for book ID mapping.")
        print("Please ensure the book_weights.csv file exists at the specified path.")
        return {}
    except Exception as e:
        print(f"Error loading book mappings from {csv_path}: {e}")
        return {}
    return book_id_mapping

def extract_outcomes_from_games(games_data):
    """Extract outcomes from the games data structure."""
    outcomes = []
    def recursive_extract(obj, parent_keys=None):
        if parent_keys is None:
            parent_keys = []
        if isinstance(obj, dict):
            if "outcome_id" in obj or ("odds" in obj and "book_id" in obj):
                outcomes.append(obj)
            elif "lines" in obj or "markets" in obj or "betting" in obj:
                for key, value in obj.items():
                    recursive_extract(value, parent_keys + [key])
            elif "books" in obj or "sportsbooks" in obj:
                for key, value in obj.items():
                    recursive_extract(value, parent_keys + [key])
            else:
                for key, value in obj.items():
                    if key in ["odds", "lines", "markets", "betting", "books", "sportsbooks", "outcomes"]:
                        recursive_extract(value, parent_keys + [key])
                    elif isinstance(value, (dict, list)):
                        recursive_extract(value, parent_keys + [key])
        elif isinstance(obj, list):
            for item in obj:
                recursive_extract(item, parent_keys)
    recursive_extract(games_data)
    return outcomes

def build_event_lookup_from_games(games_data):
    """Build a lookup dictionary for event IDs to team names and logos from games data."""
    event_lookup = {}
    def process_games(obj):
        if isinstance(obj, dict):
            if all(k in obj for k in ["id", "home_team_id", "away_team_id"]) and "teams" in obj:
                home = next((t for t in obj["teams"] if t["id"] == obj["home_team_id"]), None)
                away = next((t for t in obj["teams"] if t["id"] == obj["away_team_id"]), None)
                if home and away:
                    event_lookup[obj["id"]] = {
                        "home_team": home.get("full_name", home.get("name", "")),
                        "away_team": away.get("full_name", away.get("name", "")),
                        "home_logo": home.get("logo", ""),
                        "away_logo": away.get("logo", "")
                    }
            elif "games" in obj:
                for game in obj["games"]:
                    process_games(game)
            elif "events" in obj:
                for event in obj["events"]:
                    process_games(event)
            else:
                for value in obj.values():
                    if isinstance(value, (dict, list)):
                        process_games(value)
        elif isinstance(obj, list):
            for item in obj:
                process_games(item)
    process_games(games_data)
    return event_lookup

def extract_all_data_from_games(games_data):
    """Extract all betting data including outcomes, events, and book info from games data."""
    print("Extracting all data from games endpoint...")
    outcomes = extract_outcomes_from_games(games_data)
    print(f"Found {len(outcomes)} betting outcomes from games data")
    event_lookup = build_event_lookup_from_games(games_data)
    print(f"Built event lookup for {len(event_lookup)} events")
    if not outcomes:
        print("No outcomes found in standard format. Searching for any betting data...")
        outcomes = search_for_betting_data(games_data)
        print(f"Found {len(outcomes)} potential betting records")
    return outcomes, event_lookup

def search_for_betting_data(obj, betting_data=None):
    """Recursively search for any betting-related data structures."""
    if betting_data is None:
        betting_data = []
    if isinstance(obj, dict):
        betting_indicators = ["odds", "spread", "total", "moneyline", "book_id", "outcome_id",
                             "tickets", "money", "bet_info", "line", "value"]
        if any(indicator in obj for indicator in betting_indicators):
            betting_data.append(obj)
        for value in obj.values():
            if isinstance(value, (dict, list)):
                search_for_betting_data(value, betting_data)
    elif isinstance(obj, list):
        for item in obj:
            search_for_betting_data(item, betting_data)
    return betting_data

def save_to_csv(outcomes, event_lookup, book_id_mapping, csv_filename):
    """Save outcomes to a CSV file with team data, logos, and book names."""
    csv_fields = [
        "event_id", "home_team", "away_team", "home_logo", "away_logo",
        "book_id", "book_name", "market_type", "outcome_id",
        "event_type", "type", "side", "period", "team_id", "odds", "value", "is_live",
        "line_status", "tickets_value", "tickets_percent", "money_value", "money_percent",
        "odds_coefficient_score"
    ]
    with open(csv_filename, mode="w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=csv_fields)
        writer.writeheader()
        for outcome in outcomes:
            event_id = outcome.get("event_id")
            team_data = event_lookup.get(event_id, {
                "home_team": "", "away_team": "", "home_logo": "", "away_logo": ""
            })
            bet_info = outcome.get("bet_info", {})
            tickets = bet_info.get("tickets", {}) if isinstance(bet_info, dict) else {}
            money = bet_info.get("money", {}) if isinstance(bet_info, dict) else {}
            if not tickets and "tickets" in outcome:
                tickets = outcome["tickets"]
            if not money and "money" in outcome:
                money = outcome["money"]
            book_id = outcome.get("book_id", "")
            book_name = book_id_mapping.get(str(book_id), f"Unknown_{book_id}")
            row = {
                "event_id": event_id,
                "home_team": team_data["home_team"],
                "away_team": team_data["away_team"],
                "home_logo": team_data["home_logo"],
                "away_logo": team_data["away_logo"],
                "book_id": book_id,
                "book_name": book_name,
                "market_type": outcome.get("type", outcome.get("market_type", "")),
                "outcome_id": outcome.get("outcome_id", ""),
                "event_type": outcome.get("event_type", ""),
                "type": outcome.get("type", ""),
                "side": outcome.get("side", ""),
                "period": outcome.get("period", ""),
                "team_id": outcome.get("team_id", ""),
                "odds": outcome.get("odds", ""),
                "value": outcome.get("value", ""),
                "is_live": outcome.get("is_live", ""),
                "line_status": outcome.get("line_status", ""),
                "tickets_value": tickets.get("value", 0) if isinstance(tickets, dict) else 0,
                "tickets_percent": tickets.get("percent", 0) if isinstance(tickets, dict) else 0,
                "money_value": money.get("value", 0) if isinstance(money, dict) else 0,
                "money_percent": money.get("percent", 0) if isinstance(money, dict) else 0,
                "odds_coefficient_score": outcome.get("odds_coefficient_score", ""),
            }
            writer.writerow(row)

def separate_live_data(df, formatted_date):
    """Separate live betting data from regular data and save to separate file."""
    df['is_live'] = df['is_live'].astype(str).str.lower().isin(['true', '1', 'yes', 'y'])
    live_data = df[df['is_live'] == True].copy()
    regular_data = df[df['is_live'] == False].copy()
    if not live_data.empty:
        live_filename = f"/content/live_betting_data_{formatted_date}.csv"
        live_data.to_csv(live_filename, index=False)
        print(f"✅ Saved {len(live_data)} live betting rows to '{live_filename}'")
    else:
        print("No live betting data found")
    print(f"Remaining regular data: {len(regular_data)} rows")
    return regular_data

def format_date(date_input):
    """Convert date input to YYYYMMDD format."""
    if isinstance(date_input, str):
        try:
            for fmt in ["%Y-%m-%d", "%m/%d/%Y", "%Y%m%d"]:
                try:
                    date_obj = datetime.strptime(date_input, fmt)
                    return date_obj.strftime("%Y%m%d")
                except ValueError:
                    continue
            raise ValueError(f"Unable to parse date: {date_input}")
        except ValueError:
            raise ValueError(f"Invalid date format: {date_input}")
    elif isinstance(date_input, datetime):
        return date_input.strftime("%Y%m%d")
    else:
        raise ValueError(f"Date must be string or datetime object, got {type(date_input)}")

# --- Main Execution ---
def main(target_date=None, books_csv_path="/content/drive/MyDrive/Positive_ev/book_weights.csv"):
    """
    Fetch and process MLB betting data, saving to a combined CSV file.
    """
    # Format the date
    if target_date is None:
        formatted_date = datetime.now().strftime("%Y%m%d")
    else:
        formatted_date = format_date(target_date)
    print(f"Processing data for date: {formatted_date}")

    # Define URL
    games_url = f"https://api.actionnetwork.com/web/v2/scoreboard/publicbetting/mlb?bookIds=13,281,286,1138,1798,1921,2396,3848,4256&date={formatted_date}&periods=event"

    # Step 1: Load book ID mappings
    print(f"Loading book ID mappings from: {books_csv_path}")
    book_id_mapping = load_book_mapping(books_csv_path)

    # Exit if no book mappings were loaded
    if not book_id_mapping:
        print("Cannot proceed without book ID mappings. Please check your book_weights.csv file.")
        return None

    # Step 2: Fetch data
    print("Fetching MLB data from games endpoint...")
    games_data = fetch_json(games_url)

    # Step 3: Extract data
    outcomes, event_lookup = extract_all_data_from_games(games_data)
    if not outcomes:
        print("Warning: No betting outcomes found in games data.")
        print("This might indicate a change in the API structure.")
        print("Games data structure (first level keys):")
        if isinstance(games_data, dict):
            print(list(games_data.keys()))
        return None

    # Step 4: Save to CSV
    print("Saving outcomes to CSV with team logos and book names...")
    save_to_csv(outcomes, event_lookup, book_id_mapping, f"/content/mlb_betting_data_{formatted_date}.csv")

    # Step 5: Load and separate live data
    print("Loading data and separating live betting data...")
    df = pd.read_csv(f"/content/mlb_betting_data_{formatted_date}.csv")
    df = separate_live_data(df, formatted_date)
    df['book_name'] = df['book_name'].astype(str).fillna('')

    # Step 6: Process non-live data
    print("Processing non-live data...")
    consensus_rows = df[df["book_name"].str.contains("Consensus", na=False)]
    if not consensus_rows.empty:
        consensus_rows.to_csv(f"/content/consensus_data_{formatted_date}.csv", index=False)
        print(f"Saved {len(consensus_rows)} consensus rows")

    open_rows = df[df["book_name"].str.contains("Open", na=False)]
    if not open_rows.empty:
        open_rows.to_csv(f"/content/open_data_{formatted_date}.csv", index=False)
        print(f"Saved {len(open_rows)} open rows")

    sportsbook_data = df[~df["book_name"].str.contains("Consensus|Open", na=False)]
    match_columns = ["event_id", "market_type", "side", "value"]
    top_odds = pd.DataFrame()
    if not sportsbook_data.empty:
        top_odds = (sportsbook_data.groupby(match_columns)
                   .apply(lambda x: x.nlargest(1, "odds", keep="all"))
                   .reset_index(drop=True))
        top_odds.to_csv(f"/content/top_odds_{formatted_date}.csv", index=False)
        print(f"Saved top odds data with {len(top_odds)} rows")

    # Combine results
    combined_dfs = []
    for df_data, name in [(consensus_rows, "consensus"), (top_odds, "top_odds"), (open_rows, "open")]:
        if not df_data.empty:
            combined_dfs.append(df_data)

    combined_filename = f"/content/combined_output_{formatted_date}.csv"
    if combined_dfs:
        combined_df = pd.concat(combined_dfs, ignore_index=True)
        combined_df = combined_df.sort_values(by="event_id")
        combined_df.to_csv(combined_filename, index=False)
        print(f"✅ Combined data saved as '{combined_filename}'")

    return combined_filename

from datetime import datetime
from zoneinfo import ZoneInfo  # If using Python 3.9+

if __name__ == "__main__":
    # Get today's date in Eastern Time (EST/EDT)
    eastern_now = datetime.now(ZoneInfo("America/New_York"))
    target_date = eastern_now.strftime("%Y-%m-%d")

    # Run main logic with dynamic date
    result_file = main(target_date=target_date, books_csv_path="/content/drive/MyDrive/Positive_ev/book_weights.csv")

    if result_file:
        print(f"Processing complete. Final output: {result_file}")
    else:
        print("Processing failed - no data found or missing book mappings.")

import pandas as pd
from datetime import datetime
import pytz

# Read the CSV filesfrom datetime import datetime
from zoneinfo import ZoneInfo

# Get today's date in EST/EDT
eastern_now = datetime.now(ZoneInfo("America/New_York"))
date_str = eastern_now.strftime("%Y%m%d")

# Dynamically load the correct file
consensus_df = pd.read_csv(f"/content/combined_output_{date_str}.csv")
weighted_avg_df = pd.read_csv('/content/weighted_averages_results.csv')

# Display basic info about the files
print("Weighted Averages Results shape:", weighted_avg_df.shape)
print("Consensus Data shape:", consensus_df.shape)

# Show the first few rows to verify structure
print("\nWeighted Averages Results columns:")
print(weighted_avg_df.columns.tolist())
print("\nConsensus Data columns:")
print(consensus_df.columns.tolist())

# Filter consensus data for book_id = 15 only
consensus_filtered = consensus_df[consensus_df['book_id'] == 15].copy()
print(f"\nConsensus rows with book_id = 15: {len(consensus_filtered)}")

# Create a copy of the original consensus data to modify
updated_consensus_df = consensus_df.copy()

# Create merge keys for matching
# Convert event_id to string to ensure consistent matching
weighted_avg_df['merge_key'] = (weighted_avg_df['event_id'].astype(str) + '_' +
                               weighted_avg_df['type'].astype(str) + '_' +
                               weighted_avg_df['side'].astype(str))

consensus_filtered['merge_key'] = (consensus_filtered['event_id'].astype(str) + '_' +
                                  consensus_filtered['type'].astype(str) + '_' +
                                  consensus_filtered['side'].astype(str))

# Merge to find matching rows
merged_data = consensus_filtered.merge(
    weighted_avg_df[['merge_key', 'weighted_average_odds']],
    on='merge_key',
    how='inner'
)

print(f"\nFound {len(merged_data)} matching rows to update")

if len(merged_data) > 0:
    print("\nSample of matches found:")
    print(merged_data[['event_id', 'type', 'side', 'odds', 'weighted_average_odds']].head())

# Update the odds in the consensus dataframe
for idx, row in merged_data.iterrows():
    # Find the corresponding row in the full consensus dataframe
    mask = ((updated_consensus_df['event_id'] == row['event_id']) &
            (updated_consensus_df['type'] == row['type']) &
            (updated_consensus_df['side'] == row['side']) &
            (updated_consensus_df['book_id'] == 15))

    # Update the odds column
    updated_consensus_df.loc[mask, 'odds'] = row['weighted_average_odds']

# Generate output filename with current EST timestamp
est = pytz.timezone('US/Eastern')
current_time = datetime.now(est)
date_str = current_time.strftime('%Y%m%d')
output_filename = f'/content/combined_output_{date_str}.csv'

# Save the updated dataframe
updated_consensus_df.to_csv(output_filename, index=False)

print(f"\nUpdated consensus data saved to: {output_filename}")
print(f"Total rows in output: {len(updated_consensus_df)}")

# Show before/after comparison for verification
if len(merged_data) > 0:
    print("\nBefore/After comparison (first few matches):")
    sample_matches = merged_data.head(3)

    for _, row in sample_matches.iterrows():
        mask = ((consensus_df['event_id'] == row['event_id']) &
                (consensus_df['type'] == row['type']) &
                (consensus_df['side'] == row['side']) &
                (consensus_df['book_id'] == 15))

        original_odds = consensus_df.loc[mask, 'odds'].iloc[0]
        new_odds = row['weighted_average_odds']

        print(f"Event {row['event_id']}, {row['type']}, {row['side']}: {original_odds} → {new_odds}")

print("\nScript completed successfully!")

import pandas as pd
import pytz
from datetime import datetime
from datetime import datetime
from zoneinfo import ZoneInfo  # Use pytz if on Python < 3.9

# Get today's date in EST/EDT
eastern_now = datetime.now(ZoneInfo("America/New_York"))
date_str = eastern_now.strftime("%Y%m%d")

# Define file paths dynamically
WEIGHTED_AVG_FILE = '/content/weighted_averages_results.csv'  # Stays static
COMBINED_OUTPUT_FILE = f'/content/combined_output_{date_str}.csv'
UPDATED_COMBINED_OUTPUT_FILE = f'/content/updated_combined_output_{date_str}.csv'

print("--- Starting Match Finder and Data Update (Ignoring Book ID for Match, Updating for Book ID 15) ---")

try:
    # Read the CSV files
    weighted_avg_df = pd.read_csv(WEIGHTED_AVG_FILE)
    combined_df = pd.read_csv(COMBINED_OUTPUT_FILE)

    print(f"\nLoaded '{WEIGHTED_AVG_FILE}'. Shape: {weighted_avg_df.shape}")
    print(f"Loaded '{COMBINED_OUTPUT_FILE}'. Shape: {combined_df.shape}")

    # --- Data Cleaning and Normalization for consistent matching ---
    # Convert event_id to nullable integer to remove decimals and ensure consistency
    for df in [weighted_avg_df, combined_df]:
        if 'event_id' in df.columns:
            # Using -1 for NaN placeholder before converting to integer, then converting back to NaN for display if desired
            df['event_id'] = df['event_id'].fillna(-1).astype('Int64')
            print(f"Cleaned 'event_id' to integer type for a DataFrame.")

    # Apply strip and lower to 'type' and 'side' for robust matching
    for col in ['type', 'side']:
        if col in weighted_avg_df.columns:
            weighted_avg_df[col] = weighted_avg_df[col].astype(str).str.strip().str.lower()
        if col in combined_df.columns:
            combined_df[col] = combined_df[col].astype(str).str.strip().str.lower()
    print("Normalized 'type' and 'side' columns (strip whitespace, lowercase).")

    # --- Prepare for finding matches ---
    # Create a unique key for matching based on event_id, type, side
    weighted_avg_df['match_key'] = (
        weighted_avg_df['event_id'].astype(str) + '_' +
        weighted_avg_df['type'] + '_' +
        weighted_avg_df['side']
    )
    combined_df['match_key'] = (
        combined_df['event_id'].astype(str) + '_' +
        combined_df['type'] + '_' +
        combined_df['side']
    )

    # --- Merge to bring in weighted average data ---
    # Use a left merge to keep all rows from combined_df and add matching weighted_avg_df columns
    # Columns 'weighted_average_odds' and 'most_common_value' will be added directly to combined_df
    combined_df = pd.merge(
        combined_df,
        weighted_avg_df[['match_key', 'weighted_average_odds', 'most_common_value']],
        on='match_key',
        how='left'
    )
    print("\nMerged 'weighted_average_odds' and 'most_common_value' into combined DataFrame.")

    # --- Conditional Replacement ---
    # Identify rows in combined_df that have a match key and book_id = 15
    # The 'weighted_average_odds' column will be non-null for matched rows
    condition = (
        combined_df['book_id'] == 15
    ) & (
        combined_df['weighted_average_odds'].notna() # Check for the actual column name
    )

    # Update 'odds' and 'value' columns where the condition is true
    combined_df.loc[condition, 'odds'] = combined_df.loc[condition, 'weighted_average_odds'] # Corrected column name
    combined_df.loc[condition, 'value'] = combined_df.loc[condition, 'most_common_value']   # Corrected column name

    print(f"\nUpdated 'odds' and 'value' for {condition.sum()} rows where 'book_id' is 15 and a match was found.")

    # Drop the temporary columns after update
    combined_df = combined_df.drop(columns=['weighted_average_odds', 'most_common_value', 'match_key']) # Corrected column names to drop
    weighted_avg_df = weighted_avg_df.drop(columns=['match_key']) # Also drop from weighted_avg_df if not needed further

    # --- Save the updated combined DataFrame ---
    combined_df.to_csv(UPDATED_COMBINED_OUTPUT_FILE, index=False)
    print(f"\nUpdated data saved to '{UPDATED_COMBINED_OUTPUT_FILE}'.")
    print(f"Final shape of updated combined DataFrame: {combined_df.shape}")

except FileNotFoundError as e:
    print(f"\nERROR: One of the files was not found. Please check the path:\n{e}")
except KeyError as e:
    print(f"\nERROR: A required column was not found. Please check your CSV files for column names.\nMissing column: {e}")
except Exception as e:
    print(f"\nAn unexpected error occurred: {e}")

print("\n--- Match Finder and Data Update Completed ---")

import pandas as pd
import io
import re
from datetime import datetime
import pytz # Import pytz for timezone handling

def merge_odds_data_from_files():
    """
    Reads fair odds and book odds data from CSV files and merges them,
    adding 'FAIR_ODDS' entries. Saves the final merged DataFrame to a CSV file.

    Returns:
        pd.DataFrame: A new DataFrame with fair odds entries appended to the
                      book odds data, formatted according to the book odds structure.
    """
    # Define file paths
    fair_odds_filepath = "/content/weighted_averages_results.csv"

    # Define the Eastern Standard Time (EST) timezone
    est_timezone = pytz.timezone('America/New_York') # 'America/New_York' covers EST/EDT

    # Get the current time in EST
    now_est = datetime.now(est_timezone)

    # Generate today's date in YYYYMMDD format for the input and output file names, using EST
    today_date_str = now_est.strftime("%Y%m%d")
    book_odds_filepath = f"/content/combined_output_{today_date_str}.csv"
    output_filepath = f"/content/merged_output_{today_date_str}.csv" # New output file path

    # --- Step 1: Parse the input files into pandas DataFrames ---
    try:
        # Assuming CSV files are comma-separated by default.
        # If your files are space-separated, change sep=',' to sep=r'\s+'
        df1_fair_odds = pd.read_csv(fair_odds_filepath)
        df2_book_odds = pd.read_csv(book_odds_filepath)
    except FileNotFoundError as e:
        print(f"Error: One of the input files not found. Please ensure files exist at the specified paths: {e}")
        return pd.DataFrame() # Return empty DataFrame on error
    except Exception as e:
        print(f"Error reading CSV files. Please check file format and content: {e}")
        return pd.DataFrame() # Return empty DataFrame on error

    # Restore multi-word team names by replacing underscores with spaces
    # This assumes team names might still come with underscores from CSV
    for df in [df1_fair_odds, df2_book_odds]:
        if 'home_team' in df.columns:
            df['home_team'] = df['home_team'].astype(str).str.replace('_', ' ')
        if 'away_team' in df.columns:
            df['away_team'] = df['away_team'].astype(str).str.replace('_', ' ')

    # --- Step 2: Prepare new rows for 'FAIR_ODDS' to match df2's structure ---
    fair_odds_new_rows = []
    # Get all column names from the existing book odds DataFrame (df2)
    df2_columns = df2_book_odds.columns.tolist()

    for index, row in df1_fair_odds.iterrows():
        new_entry = {}
        # Initialize all columns that exist in df2 with "000"
        for col in df2_columns:
            new_entry[col] = "000"

        # Populate relevant fields from df1 and specific requirements
        new_entry['event_id'] = row['event_id']
        new_entry['home_team'] = row['home_team']
        new_entry['away_team'] = row['away_team']
        new_entry['book_name'] = 'FAIR_ODDS' # As per request
        new_entry['market_type'] = row['type'] # 'types' from df1 maps to 'market_type' in df2

        # Construct 'typeside' by concatenating 'types' and 'side' from df1
        new_entry['typeside'] = f"{row['type']}{row['side']}"

        new_entry['odds'] = row['weighted_average_odds'] # 'weighted_average_odds' from df1 maps to 'odds' in df2

        fair_odds_new_rows.append(new_entry)

    # Create a DataFrame from the new fair odds entries
    fair_odds_df_formatted = pd.DataFrame(fair_odds_new_rows)

    # Ensure the columns of the new DataFrame match the order of df2_book_odds
    for col in df2_columns:
        if col not in fair_odds_df_formatted.columns:
            fair_odds_df_formatted[col] = "000"

    fair_odds_df_formatted = fair_odds_df_formatted[df2_columns]

    # --- Step 3: Concatenate the DataFrames ---
    merged_dataframe = pd.concat([df2_book_odds, fair_odds_df_formatted], ignore_index=True)

    # --- Step 4: Save the merged DataFrame to a CSV file ---
    try:
        merged_dataframe.to_csv(output_filepath, index=False)
        print(f"Merged data successfully saved to {output_filepath}")
    except Exception as e:
        print(f"Error saving merged data to CSV: {e}")

    return merged_dataframe.to_string() # Still return string representation for display purposes

# Call the function to merge data from files
merged_result_string = merge_odds_data_from_files()

# Print the merged result
print(merged_result_string)

import pandas as pd
import io
import re
from datetime import datetime
import pytz # Import pytz for timezone handling

def american_to_implied_probability(odds):
    """Converts American odds to implied win probability."""
    if odds > 0:
        return 100 / (odds + 100)
    elif odds < 0:
        return abs(odds) / (abs(odds) + 100)
    else:
        return 0.5 # For even money (0 odds, though rare in American)

def american_to_decimal(odds):
    """Converts American odds to decimal odds for payout calculation."""
    if odds > 0:
        return (odds / 100) + 1
    elif odds < 0:
        return (100 / abs(odds)) + 1
    else:
        return 2.0 # For even money (0 odds)

def calculate_expected_value(fair_odds_american, sportsbook_odds_american):
    """
    Calculates Expected Value (EV) given fair odds and sportsbook odds.
    EV = (Probability_of_Win * Payout_on_Win) - (Probability_of_Loss * Stake)
    Assuming a 1 unit stake.
    """
    if fair_odds_american is None or sportsbook_odds_american is None:
        return None # Cannot calculate EV if fair odds are missing

    try:
        # P = True probability of winning (based on fair odds)
        fair_prob = american_to_implied_probability(fair_odds_american)

        # R = Return if you win (based on sportsbook odds, excluding stake)
        # This is the multiplier for your stake if you win.
        sportsbook_payout_decimal = american_to_decimal(sportsbook_odds_american)

        # EV = (fair_prob * (sportsbook_payout_decimal - 1)) - (1 - fair_prob)
        # (Probability of win * Net payout) - (Probability of loss * Stake)
        expected_value = (fair_prob * (sportsbook_payout_decimal - 1)) - (1 - fair_prob)
        return expected_value
    except Exception as e:
        print(f"Error calculating EV: {e}")
        return None


def merge_odds_data_from_files():
    """
    Reads fair odds and book odds data from CSV files and merges them,
    adding 'FAIR_ODDS' entries. Saves the final merged DataFrame to a CSV file.

    Returns:
        pd.DataFrame: A new DataFrame with fair odds entries appended to the
                      book odds data, formatted according to the book odds structure.
    """
    # Define file paths
    fair_odds_filepath = "/content/weighted_averages_results.csv"

    # Define the Eastern Standard Time (EST) timezone
    est_timezone = pytz.timezone('America/New_York') # 'America/New_York' covers EST/EDT

    # Get the current time in EST
    now_est = datetime.now(est_timezone)

    # Generate today's date in YYYYMMDD format for the input and output file names, using EST
    today_date_str = now_est.strftime("%Y%m%d")
    book_odds_filepath = f"/content/combined_output_{today_date_str}.csv"
    output_filepath = f"/content/merged_output_{today_date_str}.csv" # New output file path

    # --- Step 1: Parse the input files into pandas DataFrames ---
    try:
        # Assuming CSV files are comma-separated by default.
        # If your files are space-separated, change sep=',' to sep=r'\s+'
        df1_fair_odds = pd.read_csv(fair_odds_filepath)
        df2_book_odds = pd.read_csv(book_odds_filepath)
    except FileNotFoundError as e:
        print(f"Error: One of the input files not found. Please ensure files exist at the specified paths: {e}")
        return pd.DataFrame() # Return empty DataFrame on error
    except Exception as e:
        print(f"Error reading CSV files. Please check file format and content: {e}")
        return pd.DataFrame() # Return empty DataFrame on error

    # Restore multi-word team names by replacing underscores with spaces
    # This assumes team names might still come with underscores from CSV
    for df in [df1_fair_odds, df2_book_odds]:
        if 'home_team' in df.columns:
            df['home_team'] = df['home_team'].astype(str).str.replace('_', ' ')
        if 'away_team' in df.columns:
            df['away_team'] = df['away_team'].astype(str).str.replace('_', ' ')

    # --- Step 2: Create lookup dictionary for weighted average odds ---
    # Create a mapping from (event_id, type, side) to weighted_average_odds
    weighted_avg_lookup = {}
    for _, row in df1_fair_odds.iterrows():
        key = (row['event_id'], row['type'], row['side'])
        weighted_avg_lookup[key] = row['weighted_average_odds']

    # --- Step 3: Replace consensus odds with weighted averages ---
    # First, get consensus data for tickets_percent and money_percent
    consensus_data = df2_book_odds[df2_book_odds['book_name'].str.contains('Consensus', na=False)].copy()

    # Create modified consensus entries with weighted average odds
    modified_consensus_rows = []
    for _, row in consensus_data.iterrows():
        # Try to find matching weighted average odds
        lookup_key = (row['event_id'], row['market_type'], row['side'])
        if lookup_key in weighted_avg_lookup:
            # Create new row with weighted average odds but keep consensus tickets/money data
            new_row = row.copy()
            new_row['odds'] = weighted_avg_lookup[lookup_key]
            new_row['book_name'] = 'Weighted_Average'  # Rename to distinguish from original consensus
            modified_consensus_rows.append(new_row)
        else:
            # If no weighted average found, keep original consensus data
            modified_consensus_rows.append(row)

    # Convert to DataFrame
    modified_consensus_df = pd.DataFrame(modified_consensus_rows)

    # --- Step 4: Remove original consensus and add modified version ---
    # Remove original consensus entries
    df2_book_odds_no_consensus = df2_book_odds[~df2_book_odds['book_name'].str.contains('Consensus', na=False)]

    # Add modified consensus entries
    merged_dataframe = pd.concat([df2_book_odds_no_consensus, modified_consensus_df], ignore_index=True)

    # --- Step 5: Prepare new rows for 'FAIR_ODDS' to match df2's structure ---
    fair_odds_new_rows = []

    for index, row in df1_fair_odds.iterrows():
        new_entry = {}
        # Populate relevant fields from df1 and specific requirements
        new_entry['event_id'] = row['event_id']
        new_entry['home_team'] = row['home_team']
        new_entry['away_team'] = row['away_team']
        new_entry['book_name'] = 'FAIR_ODDS' # As per request
        new_entry['market_type'] = row['type'] # 'types' from df1 maps to 'market_type' in df2
        new_entry['side'] = row['side'] # 'side' from df1 maps to 'side' in df2

        # Populate 'typeside' by concatenating 'types' and 'side' for consistency with df2's existing data
        new_entry['typeside'] = f"{row['type']}{row['side']}"

        new_entry['odds'] = row['weighted_average_odds'] # 'weighted_average_odds' from df1 maps to 'odds' in df2

        fair_odds_new_rows.append(new_entry)

    # Create a DataFrame from the new fair odds entries
    fair_odds_df_formatted = pd.DataFrame(fair_odds_new_rows)

    # --- Step 6: Add fair odds to the merged DataFrame ---
    merged_dataframe = pd.concat([merged_dataframe, fair_odds_df_formatted], ignore_index=True)

    # Fill NaN values (which will result from columns existing in one DF but not the other) with "000"
    merged_dataframe = merged_dataframe.fillna("000")

    # Convert odds columns to numeric if they are not already, coercing errors
    for col in ['odds', 'weighted_average_odds']:
        if col in merged_dataframe.columns:
            merged_dataframe[col] = pd.to_numeric(merged_dataframe[col], errors='coerce')

    # --- Step 7: Generate formatted report (including EV calculation) ---
    try:
        generate_formatted_report(merged_dataframe, f"/content/line_movement_report_{today_date_str}.txt")
    except Exception as e:
        print(f"Error generating report: {e}")

    # --- Step 8: Save the merged DataFrame to a CSV file ---
    try:
        merged_dataframe.to_csv(output_filepath, index=False)
        print(f"Merged data successfully saved to {output_filepath}")
    except Exception as e:
        print(f"Error saving merged data to CSV: {e}")

    return merged_dataframe.to_string() # Still return string representation for display purposes

def format_date(date_input):
    """Convert date input to YYYYMMDD format."""
    if isinstance(date_input, str):
        try:
            for fmt in ["%Y-%m-%d", "%m/%d/%Y", "%Y%m%d"]:
                try:
                    date_obj = datetime.strptime(date_input, fmt)
                    return date_obj.strftime("%Y%m%d")
                except ValueError:
                    continue
            raise ValueError(f"Unable to parse date: {date_input}")
        except ValueError:
            raise ValueError(f"Invalid date format: {date_input}")
    elif isinstance(date_input, datetime):
        return date_input.strftime("%Y%m%d")
    else:
        raise ValueError(f"Date must be string or datetime object, got {type(date_input)}")

def format_odds(odds):
    """Format odds with + or - sign for readability."""
    if pd.isna(odds): # Handle NaN values which might result from '000' conversion
        return "N/A"
    return f"{odds:+.1f}" if odds > 0 else f"{odds:.1f}"

def get_favorability(open_odds, weighted_avg_odds):
    """Determine if odds movement is more or less favorable."""
    if pd.isna(open_odds) or pd.isna(weighted_avg_odds):
        return "N/A"

    if open_odds > 0 and weighted_avg_odds > 0:
        if weighted_avg_odds > open_odds:
            return "MORE favorable"
        elif weighted_avg_odds < open_odds:
            return "LESS favorable"
        else:
            return "NO change"
    elif open_odds < 0 and weighted_avg_odds < 0:
        if weighted_avg_odds > open_odds:
            return "MORE favorable"
        elif weighted_avg_odds < open_odds:
            return "LESS favorable"
        else:
            return "NO change"
    elif open_odds < 0 and weighted_avg_odds > 0:
        return "MORE favorable"
    elif open_odds > 0 and weighted_avg_odds < 0:
        return "LESS favorable"
    else:
        return "NO change"

def detect_reverse_line_movement(open_odds, weighted_avg_odds, tickets_percent, side):
    """Detect reverse line movement based on odds change and betting percentages."""
    if pd.isna(open_odds) or pd.isna(weighted_avg_odds) or pd.isna(tickets_percent):
        return False, None, 0.0

    move = weighted_avg_odds - open_odds
    if side == 'fade' and move < 0: # Fade side odds got worse (less negative or more positive)
        strength = tickets_percent
        return True, 'fade_side_became_less_favorable', strength
    return False, None, 0.0

def process_market(event_data, market_type, home_team, away_team):
    """Process data for a specific market type and return report lines."""
    if market_type == 'totals':
        market_data = event_data[event_data['market_type'] == 'total'] # Filter by 'market_type' which is 'total' for totals
    else:
        market_data = event_data[event_data['market_type'] == market_type]

    if market_data.empty:
        return []

    open_data = market_data[market_data['book_name'].str.contains('Open', na=False)]
    weighted_avg_data = market_data[market_data['book_name'].str.contains('Weighted_Average', na=False)]

    # Filter for actual sportsbooks, excluding 'Open', 'Weighted_Average', and 'FAIR_ODDS'
    sportsbook_data_filtered = market_data[~market_data['book_name'].isin(['Open', 'Weighted_Average', 'FAIR_ODDS'])]

    # Extract Fair Odds for this market (if available)
    fair_odds_market_data = market_data[market_data['book_name'] == 'FAIR_ODDS']

    if open_data.empty or weighted_avg_data.empty:
        return []

    if market_type in ['spread', 'moneyline']:
        sides = ['home', 'away']
        side_teams = {'home': home_team, 'away': away_team}
    elif market_type == 'totals':
        sides = ['over', 'under']
        side_teams = {'over': 'Over', 'under': 'Under'}
    else:
        return []

    side_data = {}
    for side_key in sides: # Use side_key to iterate through 'home', 'away', 'over', 'under'
        # Crucial fix: filter using the 'side' column directly
        side_data[side_key] = {
            'open': open_data[open_data['side'].str.contains(side_key, case=False, na=False)],
            'weighted_avg': weighted_avg_data[weighted_avg_data['side'].str.contains(side_key, case=False, na=False)],
            'sportsbook': sportsbook_data_filtered[sportsbook_data_filtered['side'].str.contains(side_key, case=False, na=False)]
        }

        # Get fair odds for the current side_key
        fair_odds_for_side = fair_odds_market_data[fair_odds_market_data['side'].str.contains(side_key, case=False, na=False)]
        if not fair_odds_for_side.empty and not pd.isna(fair_odds_for_side['odds'].iloc[0]):
            side_data[side_key]['fair_odds_value'] = fair_odds_for_side['odds'].iloc[0]
        else:
            side_data[side_key]['fair_odds_value'] = None # No fair odds for this side or it's NaN

        if side_data[side_key]['open'].empty or side_data[side_key]['weighted_avg'].empty or \
           side_data[side_key]['open']['odds'].isnull().any() or side_data[side_key]['weighted_avg']['odds'].isnull().any():
            return [] # Cannot proceed if critical odds data is missing or NaN


    side1, side2 = sides
    side1_tickets = side_data[side1]['weighted_avg']['tickets_percent'].iloc[0] if not side_data[side1]['weighted_avg'].empty else 0
    side2_tickets = side_data[side2]['weighted_avg']['tickets_percent'].iloc[0] if not side_data[side2]['weighted_avg'].empty else 0

    if side1_tickets > side2_tickets:
        public_side = side1
        fade_side = side2
    else:
        public_side = side2
        fade_side = side1

    public_tickets = side_data[public_side]['weighted_avg']['tickets_percent'].iloc[0]
    public_money = side_data[public_side]['weighted_avg']['money_percent'].iloc[0]
    fade_tickets = side_data[fade_side]['weighted_avg']['tickets_percent'].iloc[0]
    fade_money = side_data[fade_side]['weighted_avg']['money_percent'].iloc[0]

    if market_type == 'spread':
        # Assuming 'value' is consistent across sides for spread based on the line
        public_value = side_data[public_side]['weighted_avg']['value'].iloc[0]
        fade_value = side_data[fade_side]['weighted_avg']['value'].iloc[0]
        public_label = f"{side_teams[public_side]} {public_value:+.1f}"
        fade_label = f"{side_teams[fade_side]} {fade_value:+.1f}"
    elif market_type == 'totals':
        # For totals, the 'value' is the total line itself, which should be the same for 'over' and 'under'
        total_value = side_data[sides[0]]['weighted_avg']['value'].iloc[0] # Take from the first side (e.g., 'over')
        public_label = f"{side_teams[public_side]} {total_value:.1f}"
        fade_label = f"{side_teams[fade_side]} {total_value:.1f}"
    elif market_type == 'moneyline':
        public_label = f"{side_teams[public_side]} to win"
        fade_label = f"{side_teams[fade_side]} to win"
    else:
        # Fallback for unexpected market types
        public_label = f"{side_teams[public_side]}"
        fade_label = f"{side_teams[fade_side]}"


    public_open_odds = side_data[public_side]['open']['odds'].iloc[0]
    public_weighted_avg_odds = side_data[public_side]['weighted_avg']['odds'].iloc[0]
    fade_open_odds = side_data[fade_side]['open']['odds'].iloc[0]
    fade_weighted_avg_odds = side_data[fade_side]['weighted_avg']['odds'].iloc[0]

    public_move = public_weighted_avg_odds - public_open_odds
    fade_move = fade_weighted_avg_odds - fade_open_odds

    public_favor = get_favorability(public_open_odds, public_weighted_avg_odds)
    fade_favor = get_favorability(fade_open_odds, fade_weighted_avg_odds)

    rlm_detected, rlm_type, strength = detect_reverse_line_movement(
        fade_open_odds,
        fade_weighted_avg_odds,
        public_tickets, # RLM strength often judged by public betting against line movement
        'fade'
    )

    # Determine best sportsbook line for public and fade sides based on filtered data
    public_sportsbook_data_for_best = side_data[public_side]['sportsbook']
    fade_sportsbook_data_for_best = side_data[fade_side]['sportsbook']

    # Get the row with the maximum odds (best line) for each side
    best_public_row = public_sportsbook_data_for_best.loc[public_sportsbook_data_for_best['odds'].idxmax()] if not public_sportsbook_data_for_best.empty else None
    best_fade_row = fade_sportsbook_data_for_best.loc[fade_sportsbook_data_for_best['odds'].idxmax()] if not fade_sportsbook_data_for_best.empty else None

    report_lines = []
    report_lines.append(f"PUBLIC SIDE: {public_label} ({public_tickets:.0f}% tickets, {public_money:.0f}% money)")
    report_lines.append(f"FADE SIDE: {fade_label} ({fade_tickets:.0f}% tickets, {fade_money:.0f}% money)")
    report_lines.append("Line Movements:")
    report_lines.append(f"  {public_label}: {format_odds(public_move)} ({public_favor})")
    report_lines.append(f"  {fade_label}: {format_odds(fade_move)} ({fade_favor})")

    if rlm_detected:
        report_lines.append("🚨 REVERSE LINE MOVEMENT DETECTED!")
        report_lines.append(f"  Type: {rlm_type}")
        report_lines.append(f"  Strength: {strength:.1f}%")

    report_lines.append("Detailed Analysis:")
    for side_key in [fade_side, public_side]:
        current_open_odds = side_data[side_key]['open']['odds'].iloc[0]
        current_weighted_avg_odds = side_data[side_key]['weighted_avg']['odds'].iloc[0]
        current_move = current_weighted_avg_odds - current_open_odds

        current_label = ""
        if market_type == 'spread':
            current_label = f"{side_teams[side_key]} {side_data[side_key]['weighted_avg']['value'].iloc[0]:+.1f}"
        elif market_type == 'totals':
            current_label = f"{side_teams[side_key]} {total_value:.1f}"
        else: # moneyline
            current_label = f"{side_teams[side_key]} to win"
        report_lines.append(f"  {current_label}: {format_odds(current_open_odds)} → {format_odds(current_weighted_avg_odds)} (Move: {format_odds(current_move)})")

    report_lines.append("Best Sportsbook Lines:")

    # Helper to add line with EV
    def add_best_line_to_report(row, side_key, fair_odds_val, current_market_type, report_lines_list):
        if row is not None:
            book = row['book_name']
            odds = row['odds']
            ev_str = ""
            if fair_odds_val is not None:
                ev = calculate_expected_value(fair_odds_val, odds)
                if ev is not None:
                    ev_str = f" (EV: {ev:.2%})" # Format as percentage

            label_prefix = ""
            if current_market_type == 'spread':
                value = row['value'] # Use the value from the best line, not necessarily weighted average
                label_prefix = f"{side_teams[side_key]} {value:+.1f}"
            elif current_market_type == 'totals':
                value = row['value'] # Use the value from the best line
                label_prefix = f"{side_teams[side_key]} {value:.1f}"
            else: # moneyline
                label_prefix = f"{side_teams[side_key]} to win"

            report_lines_list.append(f"  {book}: {label_prefix} ({format_odds(odds)}){ev_str}")

    add_best_line_to_report(best_fade_row, fade_side, side_data[fade_side]['fair_odds_value'], market_type, report_lines)
    add_best_line_to_report(best_public_row, public_side, side_data[public_side]['fair_odds_value'], market_type, report_lines)

    return report_lines

def generate_formatted_report(df, output_path):
    """Generate a formatted line movement report including spread, moneyline, and totals."""
    # Ensure relevant columns are numeric for calculations, coercing errors
    for col in ['odds', 'value', 'tickets_percent', 'money_percent']:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')

    required_columns = ['event_id', 'home_team', 'away_team', 'book_name', 'market_type',
                        'typeside', 'side', 'odds', 'value', 'tickets_percent', 'money_percent']
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        print(f"Warning: Missing required columns for report generation: {missing_columns}")
        # Attempt to proceed but results might be incomplete

    report_lines = []
    # Filter out entries where event_id is '000' or not a valid number (e.g., from '000' fills)
    # Ensure event_id is numeric before grouping
    df_filtered_events = df[pd.to_numeric(df['event_id'], errors='coerce').notna()]
    df_filtered_events['event_id'] = pd.to_numeric(df_filtered_events['event_id'], errors='coerce')


    for event_id, event_data in df_filtered_events.groupby('event_id'):
        home_team = event_data['home_team'].iloc[0]
        away_team = event_data['away_team'].iloc[0]

        report_lines.append(f"*** {home_team} vs {away_team} (Event ID: {int(event_id)}) ***")
        report_lines.append("-" * 60)

        for market_type_key in ['spread', 'moneyline', 'totals']:
            report_lines.append(f"\n--- {market_type_key.capitalize()} Market ---")
            market_report = process_market(event_data, market_type_key, home_team, away_team)
            if market_report:
                report_lines.extend(market_report)
            else:
                report_lines.append("No sufficient data available for this market type.")
            report_lines.append("") # Add an empty line for spacing between markets

        report_lines.append("=" * 60) # Separator between events
        report_lines.append("\n") # Newline before next event for better readability

    with open(output_path, 'w') as f:
        f.write("\n".join(report_lines))
    print(f"Report saved to {output_path}")

def main():
    """Load the combined CSV and generate the line movement report."""
    merged_dataframe_str = merge_odds_data_from_files()
    # The return from merge_odds_data_from_files() is already printed inside that function
    # if you want to also print it here, you can uncomment the line below:
    # print(merged_dataframe_str)

if __name__ == "__main__":
    main()
